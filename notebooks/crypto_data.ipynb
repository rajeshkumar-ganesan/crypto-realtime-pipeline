# --- Step 1: Initialize Spark ---
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

spark = SparkSession.builder.appName("CryptoEDA").getOrCreate()

# --- Step 2: Load Historical Data ---
# Tip: Use a public CSV/Parquet dataset like 'Kaggle Bitcoin Historical Data'
df = spark.read.csv("bitcoin_historical_data.csv", header=True, inferSchema=True)

# --- Step 3: Data Cleaning & Schema Check ---
print("Total Records:", df.count())
df.printSchema()

# Convert Unix timestamp to readable date if necessary
df = df.withColumn("timestamp", from_unixtime(col("Timestamp")).cast("timestamp"))
df = df.filter(col("Close").isNotNull())

# --- Step 4: Statistical Summary ---
# Demonstrates you know how to get 'Big Data' metrics
df.select("Open", "High", "Low", "Close", "Volume_(BTC)").describe().show()

# --- Step 5: Downsampling for Visualization ---
# You can't plot 5 million rows; you must sample or aggregate
# Let's aggregate by Day to see the long-term trend
daily_df = df.groupBy(to_date("timestamp").alias("date")) \
             .agg(avg("Close").alias("avg_close"), 
                  sum("Volume_(BTC)").alias("total_volume")) \
             .orderBy("date")

# Convert to Pandas for plotting
pdf = daily_df.toPandas()

# --- Step 6: Visualizations ---
plt.figure(figsize=(12, 6))
sns.lineplot(data=pdf, x="date", y="avg_close")
plt.title("Bitcoin Average Daily Close Price")
plt.grid(True)
plt.show()

# Correlation Heatmap
corr_pdf = df.select("Open", "High", "Low", "Close", "Volume_(BTC)").sample(False, 0.1).toPandas()
plt.figure(figsize=(10, 8))
sns.heatmap(corr_pdf.corr(), annot=True, cmap='coolwarm')
plt.title("Feature Correlation Matrix")
plt.show()
